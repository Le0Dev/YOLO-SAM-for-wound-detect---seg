{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNArg0YwrVeYAeuXE/ErsPW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ow6Esja0AGkd","executionInfo":{"status":"ok","timestamp":1707770505638,"user_tz":-60,"elapsed":53829,"user":{"displayName":"Leo Dautun","userId":"01353997157044087138"}},"outputId":"6f5534cd-4734-42dc-a843-3c8e68b9af59"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q git+https://github.com/huggingface/transformers.git\n","!pip install -q datasets\n","!pip install -q monai"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","! unzip /content/drive/MyDrive/YOSAW/data_sam_wound.zip"],"metadata":{"id":"2y6FATnWAUAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import os\n","from PIL import Image\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from transformers import SamProcessor, SamModel\n","from torch.optim import Adam\n","import monai\n","from monai.losses import DiceCELoss\n","from torchvision.transforms import functional as F"],"metadata":{"id":"M0TOo4TMAUoU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_bounding_box(ground_truth_map):\n","    y_indices, x_indices = np.where(ground_truth_map > 0)\n","\n","    if len(x_indices) == 0 or len(y_indices) == 0:\n","        return [0, 0, ground_truth_map.shape[1], ground_truth_map.shape[0]]\n","\n","    x_min, x_max = np.min(x_indices), np.max(x_indices)\n","    y_min, y_max = np.min(y_indices), np.max(y_indices)\n","\n","    H, W = ground_truth_map.shape\n","    x_min = max(0, x_min - np.random.randint(0, 10))\n","    x_max = min(W, x_max + np.random.randint(0, 10))\n","    y_min = max(0, y_min - np.random.randint(0, 10))\n","    y_max = min(H, y_max + np.random.randint(0, 10))\n","\n","    bbox = [x_min, y_min, x_max, y_max]\n","\n","    return bbox"],"metadata":{"id":"TOiq9hvKAZAM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomTransform:\n","    def __init__(self, transform):\n","        self.transform = transform\n","\n","    def __call__(self, image, mask):\n","        seed = 50\n","\n","        if image is not None:\n","            torch.manual_seed(seed)\n","            image = self.transform(image)\n","\n","        if mask is not None:\n","            torch.manual_seed(seed)\n","            mask = self.transform(mask)\n","\n","        return image, mask\n","\n","class SAMDataset(Dataset):\n","    def __init__(self, images_folder, masks_folder, processor, img_size, train=True):\n","        self.images_folder = images_folder\n","        self.masks_folder = masks_folder\n","        self.processor = processor\n","        self.train = train\n","        self.img_size = img_size\n","\n","        self.transform = CustomTransform(transforms.Compose([\n","            transforms.Resize(self.img_size),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.RandomVerticalFlip(p=0.2),\n","            transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n","            #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","            #transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.1),\n","            #transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.5, p=1)], p=0.1),\n","            transforms.ToTensor(),\n","        ]))\n","\n","        self.image_files = os.listdir(images_folder)\n","        self.mask_files = os.listdir(masks_folder)\n","\n","        assert set(self.image_files) == set(self.mask_files), \"Images - masks names issues\"\n","\n","        self.image_files.sort()\n","        self.mask_files.sort()\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        image_file = self.image_files[idx]\n","        mask_file = self.mask_files[idx]\n","\n","        image_path = os.path.join(self.images_folder, image_file)\n","        mask_path = os.path.join(self.masks_folder, mask_file)\n","\n","        image = Image.open(image_path).convert(\"RGB\")\n","        mask = Image.open(mask_path).convert(\"L\")\n","\n","\n","        image, mask = self.transform(image, mask)\n","        mask = (mask > 0.5).float()\n","\n","        # Get bounding box from the transformed mask\n","        bbox = get_bounding_box(mask.squeeze().numpy())\n","\n","        prompt = bbox\n","        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs[\"ground_truth_mask\"] = mask.squeeze()\n","\n","        return inputs"],"metadata":{"id":"vlw8KsN4Aa0k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_folder = \"/content/data/train_images/\"\n","masks_folder = \"/content/data/train_masks/\"\n","val_images_folder = \"/content/data/val_images/\"\n","val_masks_folder = \"/content/data/val_masks/\"\n","\n","processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n","model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n","\n","train_dataset = SAMDataset(images_folder, masks_folder, processor,256,True)\n","val_dataset = SAMDataset(val_images_folder, val_masks_folder, processor,256,False)\n","train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=False)\n","val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=False)\n","\n","# Freeze encoder\n","for name, param in model.named_parameters():\n","    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n","        param.requires_grad_(False)\n","\n","# Initialize optimizer and loss function\n","optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n","\n","# Try DiceFocalLoss, DiceCELoss\n","seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"],"metadata":{"id":"9TBcV9dgAdg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to calculate IoU\n","def calculate_iou(predicted_masks, ground_truth_masks):\n","    intersection = torch.logical_and(predicted_masks, ground_truth_masks).sum().item()\n","    union = torch.logical_or(predicted_masks, ground_truth_masks).sum().item()\n","    iou = intersection / union if union > 0 else 0\n","    return iou"],"metadata":{"id":"HPhFFqpNCVA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 15\n","best_model_path = \"best_model.pth\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","best_iou = 0.0\n","train_losses = []\n","val_losses = []\n","train_ious = []\n","val_ious = []\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    epoch_losses = []\n","    epoch_ious = []\n","    for batch in tqdm(train_dataloader):\n","        # forward pass\n","        outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n","                        input_boxes=batch[\"input_boxes\"].to(device),\n","                        multimask_output=False)\n","\n","        # compute loss\n","        predicted_masks = outputs.pred_masks.squeeze(1)\n","        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n","        loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n","\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        # optimize\n","        optimizer.step()\n","        epoch_losses.append(loss.item())\n","\n","        # calculate IoU\n","        iou = calculate_iou(predicted_masks > 0.5, ground_truth_masks > 0.5)\n","        epoch_ious.append(iou)\n","\n","    # Store training loss and IoU\n","    train_losses.append(np.mean(epoch_losses))\n","    train_ious.append(np.mean(epoch_ious))\n","\n","    avg_train_loss = np.mean(epoch_losses)\n","    avg_train_iou = np.mean(epoch_ious)\n","    print(f\"Training Loss: {avg_train_loss:.3f}, Training IoU: {avg_train_iou:.3f}\")\n","\n","    # Validation\n","    model.eval()\n","    val_losses_epoch = []\n","    val_ious_epoch = []\n","    with torch.no_grad():\n","        for val_batch in tqdm(val_dataloader):\n","            # forward pass\n","            val_outputs = model(pixel_values=val_batch[\"pixel_values\"].to(device),\n","                                input_boxes=val_batch[\"input_boxes\"].to(device),\n","                                multimask_output=False)\n","\n","            # compute loss\n","            val_predicted_masks = val_outputs.pred_masks.squeeze(1)\n","            val_ground_truth_masks = val_batch[\"ground_truth_mask\"].float().to(device)\n","            val_loss = seg_loss(val_predicted_masks, val_ground_truth_masks.unsqueeze(1))\n","            val_losses_epoch.append(val_loss.item())\n","\n","            # calculate IoU\n","            iou = calculate_iou(val_predicted_masks > 0.5, val_ground_truth_masks > 0.5)\n","            val_ious_epoch.append(iou)\n","\n","    # Store validation loss and IoU\n","    val_losses.append(np.mean(val_losses_epoch))\n","    val_ious.append(np.mean(val_ious_epoch))\n","\n","    avg_val_loss = np.mean(val_losses_epoch)\n","    avg_val_iou = np.mean(val_ious_epoch)\n","    print(f\"Validation Loss: {avg_val_loss:.3f}, Validation IoU: {avg_val_iou:.3f}\")\n","\n","    # Save the model if it has the best IoU\n","    if avg_val_iou > best_iou:\n","        best_iou = avg_val_iou\n","        torch.save(model.state_dict(), best_model_path)\n","        print(\"Best model saved.\")"],"metadata":{"id":"RFd9tdSQCXaC"},"execution_count":null,"outputs":[]}]}